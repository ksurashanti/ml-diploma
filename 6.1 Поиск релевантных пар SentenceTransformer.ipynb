{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041a2335-f84c-46c8-9d7f-eaa61a6398c6",
   "metadata": {},
   "source": [
    "Есть датфрейм с authors, text, annotation и title. Текст предобработан, стоп-слова удалены. У одной статьи может быть несколько авторов, тогда они записаны в одной ячейке authors и разделены запятой. \n",
    "\n",
    "Функции:\n",
    "- для вычисления всех соавторов одного человека по ФИО, они могут быть записаны в разных ячейках датафрейма. \n",
    "\n",
    "- функция для векторизации текста.  \n",
    "\n",
    "- функция для вычисления коэффициента схожести.\n",
    "\n",
    "- функция для рекомендации новых соавторов. \n",
    "\n",
    "В функцию для рекомендации новых соавторов подаётся на вход ФИО автора. На выходе ФИО автора, названия всех статей в которых он участвовал, список его соавторов, список трёх рекомендуемых новых авторов/коллективов авторов, названия их статей и коэффициент близости. \n",
    "\n",
    "Ограничения:\n",
    "\n",
    "- Нельзя рекомендовать автора самому себе \n",
    "- Нельзя рекомендовать автору его текущих соавторов. \n",
    "\n",
    "Особенности:\n",
    "\n",
    "Необходимо предусмотреть ситуацию когда рекомендуется несколько человек, написавших одну и ту же статью, при совпадении коэффициента близости у новых соавторов необходимо объединять их ФИО через запятую, писать их общую статью один раз и на 2 и 3 месте писать новых рекомендуемых авторов. При совпадении 2 и 3 рекомендации также следует их объединять и на третье место выводить следующих по коэффициенту схожести коллектив авторов/автора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69a967a-a2af-4787-ba92-42d035d3c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ksurashanti\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pprint\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61969836-1e6c-4bad-b8fc-80020f94f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('date_it_lemm.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e2f2566-c10d-4eb4-b5fd-07d8fa089205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['authors'] = df['authors'].str.split(',') #авторов через запятую разделяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9adfb2c4-c44a-4a37-9a18-ebdf8ea15973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coauthors(df, author_name):\n",
    "    coauthors = set()\n",
    "    for authors_list in df['authors'].str.split(', '):\n",
    "        if author_name in authors_list:\n",
    "            coauthors.update(authors_list)\n",
    "    coauthors.discard(author_name)  # удаляем самого автора\n",
    "    return list(coauthors)\n",
    "\n",
    "def calculate_similarity(tfidf_matrix, target_indices, all_indices):\n",
    "    similarities = cosine_similarity(tfidf_matrix[target_indices], tfidf_matrix[all_indices])\n",
    "    return similarities.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eec293f1-4a84-423c-b0ae-12524c99bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_transformer_model(model_name='paraphrase-multilingual-MiniLM-L12-v2', save_path='sentence_transformer_model'):\n",
    "    # Если модель уже сохранена, загружаем её\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"Загрузка модели Sentence Transformer с диска...\")\n",
    "        return SentenceTransformer(save_path)\n",
    "    else:\n",
    "        # Иначе загружаем модель из библиотеки и сохраняем\n",
    "        print(\"Загрузка модели Sentence Transformer из библиотеки...\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        model.save(save_path)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31e500ef-4fa5-4173-8ddf-789f9e7593d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_texts_with_sentence_transformer(df, text_column='text', model_path='sentence_transformer_model'):\n",
    "    # Загружаем модель Sentence Transformer\n",
    "    model = load_sentence_transformer_model(save_path=model_path)\n",
    "    \n",
    "    # Векторизация текстов\n",
    "    print(\"Векторизация текстов...\")\n",
    "    texts = df[text_column].tolist()\n",
    "    vectors = model.encode(texts)  # Векторизация всех текстов\n",
    "    \n",
    "    return vectors, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc8be4aa-889f-436e-a7b3-3fff9ab4ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_new_coauthors(df, author_name, model_path='sentence_transformer_model'):\n",
    "    target_mask = df['authors'].str.contains(author_name, regex=False)  # Данные автора\n",
    "    target_indices = df[target_mask].index.tolist()\n",
    "    \n",
    "    if not target_indices:\n",
    "        return {\"error\": \"Автор не найден\"}\n",
    "    \n",
    "    current_coauthors = get_coauthors(df, author_name)  # Текущие соавторы\n",
    "    \n",
    "    # Векторизация текстов с помощью Sentence Transformer\n",
    "    tfidf_matrix, _ = vectorize_texts_with_sentence_transformer(df, text_column='text', model_path=model_path)\n",
    "    \n",
    "    all_indices = df.index.tolist()\n",
    "    similarities = calculate_similarity(tfidf_matrix, target_indices, all_indices)  # Схожести\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'index': all_indices,\n",
    "        'similarity': similarities,\n",
    "        'authors': df['authors'],\n",
    "        'title': df['title']\n",
    "    })\n",
    "    \n",
    "    results = results[\n",
    "        (results['similarity'] < 1.0) &  # Исключаем статьи автора\n",
    "        (~results['authors'].isin([author_name]))  # Исключаем соавторов\n",
    "    ]\n",
    "    \n",
    "    # Группировка по коэффициенту схожести\n",
    "    grouped = results.groupby('similarity').agg({\n",
    "        'authors': lambda x: ', '.join(sorted(set(', '.join(x).split(', ')))),\n",
    "        'title': lambda x: ', '.join(sorted(set(x)))\n",
    "    }).reset_index().sort_values('similarity', ascending=False)\n",
    "    \n",
    "    recommendations = []\n",
    "    used_authors = set(current_coauthors + [author_name])  # Формируем рекомендации\n",
    "    \n",
    "    for _, row in grouped.iterrows():\n",
    "        candidates = [a.strip() for a in row['authors'].split(',') if a.strip() not in used_authors]\n",
    "        if candidates:\n",
    "            unique_candidates = []\n",
    "            for candidate in candidates:\n",
    "                if candidate not in used_authors:\n",
    "                    unique_candidates.append(candidate)\n",
    "                    used_authors.add(candidate)\n",
    "            \n",
    "            if unique_candidates:\n",
    "                recommendations.append({\n",
    "                    'authors': ', '.join(unique_candidates),\n",
    "                    'titles': row['title'],\n",
    "                    'similarity': round(row['similarity'], 2)\n",
    "                })\n",
    "        \n",
    "        if len(recommendations) >= 3:\n",
    "            break\n",
    "    \n",
    "    # Итоговый вывод\n",
    "    output = {\n",
    "        \"Автор\": author_name,\n",
    "        \"Текущие соавторы\": current_coauthors,\n",
    "        \"Статьи\": df[target_mask]['title'].tolist(),\n",
    "        \"Рекомендуемые соавторы\": recommendations[:3]\n",
    "    }\n",
    "    \n",
    "    # Сохраняем результаты в файлы\n",
    "    save_results_to_txt(author_name, current_coauthors, output[\"Статьи\"], output[\"Рекомендуемые соавторы\"], df)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37873d61-4903-4cbb-902f-fc52d255da58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели Sentence Transformer из библиотеки...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470846c2d4f54af1846ebaec48aa1a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksurashanti\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ksurashanti\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa262e55259452096f77c91bdc27e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350729c1efd54de7818cdf5f2fa8445f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.89k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5b868d1973416aa3d03213b34c3997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef31be1f7ee495a8fb9da2aeacfd5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7790acb7fb7479e9c14eb47a5f80c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00011adc66674a238ab28c41648e1e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f8e28998b041e7851de2bd486970c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bd5c1effce4eebb79548d45807a60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e637015d2b02490a88414395d3af1e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Загружаем модель Sentence Transformer (она автоматически сохранится на диск)\n",
    "model = load_sentence_transformer_model(save_path='sentence_transformer_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e76a17f-9de3-4b99-ad69-cab550bcb333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_txt(author_name, current_coauthors, articles, recommendations, df):\n",
    "    # Создаем папку для сохранения файлов (если её нет)\n",
    "    import os\n",
    "    if not os.path.exists(\"output\"):\n",
    "        os.makedirs(\"output\")\n",
    "\n",
    "    # Файл 1: Информация об авторе, его соавторах и нулевой статье\n",
    "    with open(f\"output/{author_name}_info.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Автор: {author_name}\\n\")\n",
    "        f.write(f\"Соавторы: {', '.join(current_coauthors)}\\n\\n\")\n",
    "        \n",
    "        # Проверяем, есть ли статьи у автора\n",
    "        if articles:\n",
    "            try:\n",
    "                first_article = df[df['title'] == articles[0]].iloc[0]\n",
    "                f.write(f\"Заголовок: {first_article['title']}\\n\")\n",
    "                f.write(f\"Аннотация: {first_article['annotation']}\\n\")\n",
    "                f.write(f\"Текст: {first_article['text']}\\n\")\n",
    "            except IndexError:\n",
    "                f.write(\"Нулевая статья не найдена.\\n\")\n",
    "        else:\n",
    "            f.write(\"Статьи автора не найдены.\\n\")\n",
    "\n",
    "    # Файлы 2-4: Рекомендуемые статьи\n",
    "    for i, rec in enumerate(recommendations[:3], start=1):\n",
    "        with open(f\"output/{author_name}_recommendation_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Коэффициент схожести: {rec['similarity']}\\n\")\n",
    "            f.write(f\"Рекомендуемые авторы: {rec['authors']}\\n\\n\")\n",
    "            \n",
    "            titles = rec['titles'].split(', ')\n",
    "            for title in titles:\n",
    "                try:\n",
    "                    article = df[df['title'] == title].iloc[0]\n",
    "                    f.write(f\"Заголовок: {article['title']}\\n\")\n",
    "                    f.write(f\"Аннотация: {article['annotation']}\\n\")\n",
    "                    f.write(f\"Текст: {article['text']}\\n\\n\")\n",
    "                except IndexError:\n",
    "                    f.write(f\"Статья с заголовком '{title}' не найдена.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6720cb3a-47e0-48ef-85b7-84070ac1ac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели Sentence Transformer с диска...\n",
      "Векторизация текстов...\n",
      "{'Автор': 'Большаков А. О.',\n",
      " 'Рекомендуемые соавторы': [{'authors': 'Колесникова Ольга Валерьевна, Лелюхин '\n",
      "                                        'Владимир Егорович, Осипова Марина '\n",
      "                                        'Анатольевна, Цициашвилли Гурами '\n",
      "                                        'Шалвович',\n",
      "                             'similarity': 0.94,\n",
      "                             'titles': 'Формальная интерпретация задачи поиска '\n",
      "                                       'технологических баз и синтеза '\n",
      "                                       'последовательности обработки '\n",
      "                                       'поверхностей детали'},\n",
      "                            {'authors': 'Жабин Я. О., Леонтьев А. C.',\n",
      "                             'similarity': 0.94,\n",
      "                             'titles': 'АНАЛИЗ ПРОЦЕССА ПОИСКА ИГРОКОВ И '\n",
      "                                       'ФОРМИРОВАНИЯ КОМАНД ДЛЯ '\n",
      "                                       'МНОГОПОЛЬЗОВАТЕЛЬСКИХ СОРЕВНОВАТЕЛЬНЫХ '\n",
      "                                       'ИГР'},\n",
      "                            {'authors': 'Бабкин А. А., Крюкова Д. Ю., '\n",
      "                                        'Панфилова О. А.',\n",
      "                             'similarity': 0.94,\n",
      "                             'titles': 'Вопросы методического сопровождения '\n",
      "                                       'курсов \"информационная безопасность\" и '\n",
      "                                       '\"информационное обеспечение '\n",
      "                                       'государственных закупок\" в системе '\n",
      "                                       'вузовского ведомственного '\n",
      "                                       'образования'}],\n",
      " 'Статьи': ['АВТОМАТИЗАЦИЯ ПРОЦЕССОВ МОНИТОРИНГА И ИНВЕНТАРИЗАЦИИ '\n",
      "            'ИНФОРМАЦИОННО-ТЕХНОЛОГИЧЕСКОЙ ИНФРАСТРУКТУРЫ, ПРИМЕНЯЕМОЙ В '\n",
      "            'УЧЕБНОМ ПРОЦЕССЕ'],\n",
      " 'Текущие соавторы': ['Леонтьев А. С.']}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# Пример вызова функции\n",
    "rec_list = recommend_new_coauthors(df, 'Большаков А. О.', model_path='sentence_transformer_model')\n",
    "pprint.pprint(rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cf01af-ff29-460b-b30d-a5db05961a1b",
   "metadata": {},
   "source": [
    "Вывод: здесь совершенно не подходит статья про обработку поверхностей детали, а она самая рекомендуемая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14350814-1c8b-4030-8ee9-ae775058d68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
