{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6333dfbc-58ab-4d4f-8d3c-10a4411b1bae",
   "metadata": {},
   "source": [
    "0. есть датафрейм df, колонки \"title\" - название статьи, \"annotation\" - аннотация, \"text\" - текст статьи, \"authors\" - авторы\n",
    "1. Векторизация текстов с учётом семантических особенностей\n",
    "2. Кластеризация (нужны небольшие кластеры)\n",
    "3. Тестирование: выводим статью, смотрим на те же, что в кластере."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf1eb1f-1157-460b-92ae-3669efa8c2f9",
   "metadata": {},
   "source": [
    "SentenceTransformer - работает с предложениями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "592d4d2f-deb6-43fc-b76e-eb9baf632296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83eb32df-f2ad-40f9-a274-e291bc645f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('date_it_lemm.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73924335-cb2d-4464-bae2-496bcdf2bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas() #применяем прогресс-бар для пандаса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22861176-40a7-4f0a-9197-f0c052ba4ddb",
   "metadata": {},
   "source": [
    "### 1. SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3c68354-7d20-4a36-bf7a-a4f82bb9da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2308/2308 [01:55<00:00, 20.06it/s]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') #предобученная модель\n",
    "\n",
    "df['text_embedding'] = df['text'].progress_apply(lambda x: model.encode(x)) #векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9014cd25-01bb-48c9-9f23-0540ca9d787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(df['text_embedding'].tolist()) #эмбеддинги в массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edf4329f-af29-42e2-9cb8-5c64b7fb9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#диапазон кластеров здесь напишу чтобы удобно было менять, а не в коде\n",
    "min_cluster_size = 3\n",
    "max_cluster_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a3ba2-bd27-4a82-92eb-2a48c20fead6",
   "metadata": {},
   "source": [
    "#### 1.1 Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48a527da-654b-4ce6-860d-fcb2ded1d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=n_clusters,linkage='average')\n",
    "df['cluster'] = clustering.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80da650a-904e-4184-aae4-7099f0c47ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster\n",
       "28     55\n",
       "52     36\n",
       "45     28\n",
       "366    26\n",
       "83     26\n",
       "       ..\n",
       "389     1\n",
       "501     1\n",
       "692     1\n",
       "444     1\n",
       "244     1\n",
       "Name: count, Length: 769, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_sizes = df['cluster'].value_counts()\n",
    "cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fdc451f6-a350-476b-8057-2a3ddf38d932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: ПРОЦЕДУРЫ ПРИМЕНЕНИЯ АДАПТИВНЫХ СИТУАЦИОННЫХ ТРЕНАЖЕРОВ\n",
      "Authors: Удальцов Николай Петрович, Агеев Павел Александрович, Михейкина Елена Викторовна\n",
      "Annotation: В статье рассматриваются основные процедуры применения адаптивных ситуационных тренажеров на основе моделирования деятельности должностных лиц органов управления с целью совершенствования способов и методов работы и повышения качества управления.\n",
      "Text: процедура применение адаптивный ситуационный тренажёр н п удальцов п агеев михейкина статья рассматриваться основной процедура применение адаптивный ситуационный тренажёр основа моделирование деятельность должностной лицо орган управление цель совершенствование способ метод работа повышение качество управление ключевой слово должностной лицо дежурный смена орган пункт управление тренажёр моделирование обучение подготовка должностной лицо орган управление различный уровень кропотливый сложный про...\n",
      "\n",
      "Статьи из того же кластера:\n",
      "Title: Моделирование в психологических, социальных и медицинских системах с использованием методов искусственного интеллекта, Authors: Арзамасцев Александр Анатольевич, Зенкова Наталья Александровна, Неудахин Александр Викторович\n",
      "Title: Метакогнитивная модель социального тревожного расстройства, Authors: Сагалакова Ольга Анатольевна, Труевцев Дмитрий Владимирович\n",
      "Title: Имитационное моделирование в системе Rand Model Designer, Authors: Якимов И. М., Кирпичников А. П., Халиуллин Р. Ф., Мальцев С. А., Ситдиков М. Ш.\n",
      "Title: ПРОЦЕДУРЫ ПРИМЕНЕНИЯ АДАПТИВНЫХ СИТУАЦИОННЫХ ТРЕНАЖЕРОВ, Authors: Удальцов Николай Петрович, Агеев Павел Александрович, Михейкина Елена Викторовна\n",
      "Title: Моделирование сложных систем в имитационной среде AnyLogic, Authors: Якимов И. М., Кирпичников А. П., Мокшин В. В.\n",
      "Title: Имитационное моделирование сложных систем средствами Aris Toolset 6, Authors: Якимов И. М., Кирпичников А. П., Матвеева С. В., Мокшин В. В., Фролова К. А.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random_index = random.randint(0, len(df) - 1) #случайно статью выберем\n",
    "random_article = df.iloc[random_index]\n",
    "\n",
    "print(f\"Title: {random_article['title']}\")\n",
    "print(f\"Authors: {random_article['authors']}\")\n",
    "print(f\"Annotation: {random_article['annotation']}\")\n",
    "print(f\"Text: {random_article['text'][:500]}...\")  # Выводим первые 500 символов текста\n",
    "\n",
    "cluster_id = random_article['cluster'] #статьи из того же кластера\n",
    "cluster_articles = df[df['cluster'] == cluster_id]\n",
    "\n",
    "print(\"\\nСтатьи из того же кластера:\")\n",
    "for _, article in cluster_articles.iterrows():\n",
    "    print(f\"Title: {article['title']}, Authors: {article['authors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f3e86-3337-4a97-8e6c-577fd2bd6aef",
   "metadata": {},
   "source": [
    "Вообще не похоже. Попробуем другие алгоритмы кластеризации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ed0fcc-2c26-413c-ae36-422dad0b8afe",
   "metadata": {},
   "source": [
    "#### 1.2 DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ff7e4ff-c60a-4706-aace-f957496ed7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры кластеров:\n",
      "cluster\n",
      " 0     1352\n",
      "-1      798\n",
      " 9       17\n",
      " 22      10\n",
      " 3        6\n",
      "       ... \n",
      " 24       2\n",
      " 27       2\n",
      " 28       2\n",
      " 29       2\n",
      " 61       2\n",
      "Name: count, Length: 63, dtype: int64\n",
      "Количество статей, не попавших в кластеры (шум): 798\n"
     ]
    }
   ],
   "source": [
    "eps = 0.07 #максимальное расстояние между двумя образцами для того, чтобы они считались соседями\n",
    "min_samples = 2  #минимальное количество образцов в окрестности точки, чтобы она считалась ядром\n",
    "\n",
    "clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "df['cluster'] = clustering.fit_predict(embeddings)\n",
    "\n",
    "cluster_sizes = df['cluster'].value_counts()\n",
    "print(\"Размеры кластеров:\")\n",
    "print(cluster_sizes)\n",
    "\n",
    "if -1 in cluster_sizes:\n",
    "    print(f\"Количество статей, не попавших в кластеры (шум): {cluster_sizes[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770f1cc4-f651-4a9a-9ea0-59d237e3cc2c",
   "metadata": {},
   "source": [
    "В DBSCAN как ни крути параметры, всё равно либо очень много шума, либо очень много статей в одном кластере. В плане сглаженности размеров кластеров аггломеративная лучше была."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51970fe-3fa1-4626-9679-eb5f9870cbaa",
   "metadata": {},
   "source": [
    "#### 1.3 K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e0e9c31a-0a2f-4a1d-a9ce-f9f4543df56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество кластеров: 288\n"
     ]
    }
   ],
   "source": [
    "n_clusters = len(df) // 8\n",
    "print(f\"Количество кластеров: {n_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4f20e8d8-b762-409f-af59-dbcf58a52459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры кластеров:\n",
      "cluster\n",
      "195    27\n",
      "137    25\n",
      "178    24\n",
      "131    22\n",
      "63     22\n",
      "       ..\n",
      "286     1\n",
      "250     1\n",
      "201     1\n",
      "129     1\n",
      "231     1\n",
      "Name: count, Length: 288, dtype: int64\n",
      "Количество кластеров с ровно 1 элементом: 20\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "cluster_sizes = df['cluster'].value_counts()\n",
    "print(\"Размеры кластеров:\")\n",
    "print(cluster_sizes)\n",
    "\n",
    "cluster_sizes = df['cluster'].value_counts()\n",
    "\n",
    "single_item_clusters = (cluster_sizes == 1).sum()\n",
    "\n",
    "# Выводим результат\n",
    "print(f\"Количество кластеров с ровно 1 элементом: {single_item_clusters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4b736111-a5e6-4b8d-afba-4024ba66eb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: МОДЕЛИРОВАНИЕ РАБОТЫ СЕРВИСА БЫСТРОГО РЕАГИРОВАНИЯ ДЛЯ ОБЕСПЕЧЕНИЯ РАБОТЫ ТЕРМИНАЛЬНО-СКЛАДСКИХ КОМПЛЕКСОВ\n",
      "Authors: Упырь Роман Юрьевич, Дудакова Анастасия Владимировна\n",
      "Annotation: Сервис быстрого реагирования-современный тренд во многих областях деятельности, в том числе и на железнодорожном транспорте. С помощью выездных мобильных бригад проводят техническое обслуживание вагонов, их коммерческий осмотр, погрузочно-разгрузочные операции, оформление документов и др. В статье...\n",
      "Text: р ю упырь дудаков иркутский государственный университет путь сообщение иргупс г иркутск российский федерация моделирование работа сервис быстрый реагирование обеспечение работа терминальный складской комплекс аннотагщть сервис быстрый реагирование современный тренд многий область деятельность число железнодорожный транспорт помощть выездной мобильный бригада проводить технический обслуживание вагон коммерческий осмотр погрузочный разгрузочный операция оформление документ др статья пример восточн...\n",
      "\n",
      "Статьи из того же кластера:\n",
      "Title: Роль биткоинов в экономике и их производство, Authors: Вахранев Антон Владимирович\n",
      "Title: МОДЕЛИРОВАНИЕ РАБОТЫ СЕРВИСА БЫСТРОГО РЕАГИРОВАНИЯ ДЛЯ ОБЕСПЕЧЕНИЯ РАБОТЫ ТЕРМИНАЛЬНО-СКЛАДСКИХ КОМПЛЕКСОВ, Authors: Упырь Роман Юрьевич, Дудакова Анастасия Владимировна\n",
      "Title: КАК ДАННЫЕ ОБРАТНОЙ СВЯЗИ ПОЛЬЗОВАТЕЛЕЙ МЕНЯЮТ ДИЗАЙН ПРОДУКТА, Authors: Чачис Д. Ю.\n",
      "Title: Обеспечение информационной безопасности предприятий, Authors: Кирильчук С. П., Наливайченко Е. В.\n",
      "Title: Основы теории развития государства, Authors: Булюктов Б. М.\n",
      "Title: Системы безопасности объекта новая история и тенденции развития, Authors: Степин Дмитрий Анатольевич, Фадеев Юрий Иванович\n",
      "Title: СРАВНИТЕЛЬНЫЙ анализ производительности SQL И NOSQL СУБД, Authors: Новиков Борис Асенович\n",
      "Title: Алгоритм определения источника фрагментированных сообщений, Authors: Таныгин М. О.\n",
      "Title: МОДЕЛИРОВАНИЕ РАБОТЫ СЕРВИСА БЫСТРОГО РЕАГИРОВАНИЯ ДЛЯ ОБЕСПЕЧЕНИЯ РАБОТЫ ТЕРМИНАЛЬНО-СКЛАДСКИХ КОМПЛЕКСОВ, Authors: Упырь Роман Юрьевич, Дудакова Анастасия Владимировна\n",
      "Title: Построение модели солнечной радиации на основе наземных наблюдений на примере данных по Иркутску, Authors: Кузнецов Борис Федорович, Бузунова Марина Юрьевна, Бузунов Дмитрий Сергеевич\n",
      "Title: ПРИМЕНЕНИЕ СТЕНДОВ ДЛЯ ПРОВЕРКИ И ОТЛАДКИ СИСТЕМ АВТОМАТИЗАЦИИ НА БАЗЕ ПРОГРАММИРУЕМЫХ ЛОГИЧЕСКИХ КОНТРОЛЛЕРОВ, Authors: Журавский Д. Ю.\n",
      "Title: МОДЕЛЬ ПРОГНОЗИРОВАНИЯ ЖИВОГО ВЕСА С ПОМОЩЬЮ ГЛУБОКОЙ РЕГРЕССИИ RGB-D ИЗОБРАЖЕНИЙ, Authors: Ручай Алексей Николаевич\n",
      "Title: Применение концепции IP-storage для создания распределенных систем хранения данных высокой степени готовности, Authors: Фадеев А. Ю.\n",
      "Title: Нарушение энтропийного гомеостаза в органах-мишенях, Authors: Фролов В. А., Зотова Т. Ю., Зотов А. К.\n",
      "Title: МЕТОДЫ АНАЛИЗА БОЛЬШИХ ДАННЫХ В ЭКОНОМИКЕ, Authors: Алексей Александрович Микрюков, Михаил Георгиевич Гранатов, Зульфия Амуровна Абдрахманова\n"
     ]
    }
   ],
   "source": [
    "random_index = random.randint(0, len(df) - 1)\n",
    "random_article = df.iloc[random_index]\n",
    "\n",
    "print(f\"Title: {random_article['title']}\")\n",
    "print(f\"Authors: {random_article['authors']}\")\n",
    "print(f\"Annotation: {random_article['annotation']}\")\n",
    "print(f\"Text: {random_article['text'][:500]}...\")  # Выводим первые 500 символов текста\n",
    "\n",
    "cluster_id = random_article['cluster']\n",
    "cluster_articles = df[df['cluster'] == cluster_id]\n",
    "\n",
    "print(\"\\nСтатьи из того же кластера:\")\n",
    "for _, article in cluster_articles.iterrows():\n",
    "    print(f\"Title: {article['title']}, Authors: {article['authors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22b87d-f1dc-4b8f-98f8-fddbce2bfe86",
   "metadata": {},
   "source": [
    "И снова смысла достаточно мало."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d787cc-ceb0-4dcc-b681-92ec5b320c8a",
   "metadata": {},
   "source": [
    "### 2. FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "be98228e-23a2-437b-baf4-2f24002b15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "48b3b7ba-79e0-4482-9e7a-323fd85d3b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksurashanti\\AppData\\Local\\Temp\\ipykernel_7148\\1782492091.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  model = FastText.load_fasttext_format('cc.ru.300.bin')\n"
     ]
    }
   ],
   "source": [
    "model = FastText.load_fasttext_format('cc.ru.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a8cc5e0e-fc8c-42f5-b1cd-bf9b97a62e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    words = text.split()\n",
    "    word_vectors = [model.wv(word) for word in words]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(model.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d0cf2fee-c1a8-4967-84b4-b471ef847b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FastText in module gensim.models.fasttext:\n",
      "\n",
      "class FastText(gensim.models.word2vec.Word2Vec)\n",
      " |  FastText(sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      FastText\n",
      " |      gensim.models.word2vec.Word2Vec\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, epochs=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), max_final_vocab=None, shrink_windows=True)\n",
      " |      Train, use and evaluate word representations learned using the method\n",
      " |      described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_,\n",
      " |      aka FastText.\n",
      " |\n",
      " |      The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the\n",
      " |      original Fasttext implementation via :func:`~gensim.models.fasttext.load_facebook_model`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus'\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such\n",
      " |          examples. If you don't supply `sentences`, the model is left uninitialized -- use if you plan to\n",
      " |          initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left\n",
      " |          uninitialized).\n",
      " |      min_count : int, optional\n",
      " |          The model ignores all words with total frequency lower than this.\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      sg : {1, 0}, optional\n",
      " |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n",
      " |      hs : {1,0}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {1,0}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n",
      " |\n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |\n",
      " |      sorted_vocab : {1,0}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      min_n : int, optional\n",
      " |          Minimum length of char n-grams to be used for training word representations.\n",
      " |      max_n : int, optional\n",
      " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
      " |          lesser than `min_n` to avoid char ngrams being used.\n",
      " |      word_ngrams : int, optional\n",
      " |          In Facebook's FastText, \"max length of word ngram\" - but gensim only supports the\n",
      " |          default of 1 (regular unigram word handling).\n",
      " |      bucket : int, optional\n",
      " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
      " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
      " |          The default value of 2000000 consumes as much memory as having 2000000 more in-vocabulary\n",
      " |          words in your model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      max_final_vocab : int, optional\n",
      " |          Limits the vocab to a target vocab size by automatically selecting\n",
      " |          ``min_count```.  If the specified ``min_count`` is more than the\n",
      " |          automatically calculated ``min_count``, the former will be used.\n",
      " |          Set to ``None`` if not required.\n",
      " |      shrink_windows : bool, optional\n",
      " |          New in 4.1. Experimental.\n",
      " |          If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      " |          for each target word during training, to match the original word2vec algorithm's\n",
      " |          approximate weighting of context words by distance. Otherwise, the effective\n",
      " |          window size is always fixed to `window` words to either side.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a `FastText` model:\n",
      " |\n",
      " |      .. sourcecode:: pycon\n",
      " |\n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = FastText(sentences, min_count=1)\n",
      " |          >>> say_vector = model.wv['say']  # get vector for word\n",
      " |          >>> of_vector = model.wv['of']  # get vector for out-of-vocab word\n",
      " |\n",
      " |      Attributes\n",
      " |      ----------\n",
      " |      wv : :class:`~gensim.models.fasttext.FastTextKeyedVectors`\n",
      " |          This object essentially contains the mapping between words and embeddings. These are similar to\n",
      " |          the embedding computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also\n",
      " |          include vectors for n-grams. This allows the model to compute embeddings even for **unseen**\n",
      " |          words (that do not exist in the vocabulary), as the aggregate of the n-grams included in the word.\n",
      " |          After training the model, this attribute can be used directly to query those embeddings in various\n",
      " |          ways. Check the module level docstring for some examples.\n",
      " |\n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate memory that will be needed to train a model, and print the estimates to log.\n",
      " |\n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors. Obsoleted.\n",
      " |\n",
      " |      If you need a single unit-normalized vector for some key, call\n",
      " |      :meth:`~gensim.models.keyedvectors.KeyedVectors.get_vector` instead:\n",
      " |      ``fasttext_model.wv.get_vector(key, norm=True)``.\n",
      " |\n",
      " |      To refresh norms after you performed some atypical out-of-band vector tampering,\n",
      " |      call `:meth:`~gensim.models.keyedvectors.KeyedVectors.fill_norms()` instead.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original trained vectors and only keep the normalized ones.\n",
      " |          You lose information if you do this.\n",
      " |\n",
      " |  load_binary_data(self, encoding='utf8')\n",
      " |      Load data from a binary file created by Facebook's native FastText.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      encoding : str, optional\n",
      " |          Specifies the encoding.\n",
      " |\n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the Fasttext model. This saved model can be loaded again using\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n",
      " |      and getting vectors for out-of-vocabulary words.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Store the model to this file.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`\n",
      " |          Load :class:`~gensim.models.fasttext.FastText` model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  load(*args, **kwargs)\n",
      " |      Load a previously saved `FastText` model.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastText`\n",
      " |          Loaded model.\n",
      " |\n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastText` model.\n",
      " |\n",
      " |  load_fasttext_format(model_file, encoding='utf8')\n",
      " |      Deprecated.\n",
      " |\n",
      " |      Use :func:`gensim.models.fasttext.load_facebook_model` or\n",
      " |      :func:`gensim.models.fasttext.load_facebook_vectors` instead.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.word2vec.Word2Vec:\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Human readable representation of the model's state.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the model's state, including the vocabulary size, vector size\n",
      " |          and learning rate.\n",
      " |\n",
      " |  add_null_word(self)\n",
      " |\n",
      " |  build_vocab(self, corpus_iterable=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int, optional\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, the raw vocabulary will be deleted after the scaling is done to free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |\n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |\n",
      " |      **kwargs : object\n",
      " |          Keyword arguments propagated to `self.prepare_vocab`.\n",
      " |\n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |\n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |\n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |\n",
      " |  create_binary_tree(self)\n",
      " |      Create a `binary Huffman tree <https://en.wikipedia.org/wiki/Huffman_coding>`_ using stored vocabulary\n",
      " |      word counts. Frequent words will have shorter binary codes.\n",
      " |      Called internally from :meth:`~gensim.models.word2vec.Word2VecVocab.build_vocab`.\n",
      " |\n",
      " |  get_latest_training_loss(self)\n",
      " |      Get current value of the training loss.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Current training loss.\n",
      " |\n",
      " |  init_weights(self)\n",
      " |      Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.\n",
      " |\n",
      " |  make_cum_table(self, domain=2147483647)\n",
      " |      Create a cumulative-distribution table using stored vocabulary word counts for\n",
      " |      drawing random words in the negative-sampling training routines.\n",
      " |\n",
      " |      To draw a word index, choose a random integer up to the maximum value in the table (cum_table[-1]),\n",
      " |      then finding that integer's sorted insertion point (as if by `bisect_left` or `ndarray.searchsorted()`).\n",
      " |      That insertion point is the drawn index, coming up in proportion equal to the increment at that slot.\n",
      " |\n",
      " |  predict_output_word(self, context_words_list, topn=10)\n",
      " |      Get the probability distribution of the center word given context words.\n",
      " |\n",
      " |      Note this performs a CBOW-style propagation, even in SG models,\n",
      " |      and doesn't quite weight the surrounding words the same as in\n",
      " |      training -- so it's just one crude way of using a trained model\n",
      " |      as a predictor.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      context_words_list : list of (str and/or int)\n",
      " |          List of context words, which may be words themselves (str)\n",
      " |          or their index in `self.wv.vectors` (int).\n",
      " |      topn : int, optional\n",
      " |          Return `topn` words and their probabilities.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float)\n",
      " |          `topn` length list of tuples of (word, probability).\n",
      " |\n",
      " |  prepare_vocab(self, update=False, keep_raw_vocab=False, trim_rule=None, min_count=None, sample=None, dry_run=False)\n",
      " |      Apply vocabulary settings for `min_count` (discarding less-frequent words)\n",
      " |      and `sample` (controlling the downsampling of more-frequent words).\n",
      " |\n",
      " |      Calling with `dry_run=True` will only simulate the provided settings and\n",
      " |      report the size of the retained vocabulary, effective corpus length, and\n",
      " |      estimated memory requirements. Results are both printed via logging and\n",
      " |      returned as a dict.\n",
      " |\n",
      " |      Delete the raw vocabulary after the scaling is done to free up RAM,\n",
      " |      unless `keep_raw_vocab` is set.\n",
      " |\n",
      " |  prepare_weights(self, update=False)\n",
      " |      Build tables and model weights based on final vocabulary settings.\n",
      " |\n",
      " |  reset_from(self, other_model)\n",
      " |      Borrow shareable pre-built structures from `other_model` and reset hidden layer weights.\n",
      " |\n",
      " |      Structures copied are:\n",
      " |          * Vocabulary\n",
      " |          * Index to word mapping\n",
      " |          * Cumulative frequency table (used for negative sampling)\n",
      " |          * Cached corpus length\n",
      " |\n",
      " |      Useful when testing multiple models on the same corpus in parallel. However, as the models\n",
      " |      then share all vocabulary-related structures other than vectors, neither should then\n",
      " |      expand their vocabulary (which could leave the other in an inconsistent, broken state).\n",
      " |      And, any changes to any per-word 'vecattr' will affect both models.\n",
      " |\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.word2vec.Word2Vec`\n",
      " |          Another model to copy the internal structures from.\n",
      " |\n",
      " |  scan_vocab(self, corpus_iterable=None, corpus_file=None, progress_per=10000, workers=None, trim_rule=None)\n",
      " |\n",
      " |  score(self, sentences, total_sentences=1000000, chunksize=100, queue_factor=2, report_delay=1)\n",
      " |      Score the log probability for a sequence of sentences.\n",
      " |      This does not change the fitted model in any way (see :meth:`~gensim.models.word2vec.Word2Vec.train` for that).\n",
      " |\n",
      " |      Gensim has currently only implemented score for the hierarchical softmax scheme,\n",
      " |      so you need to have run word2vec with `hs=1` and `negative=0` for this to work.\n",
      " |\n",
      " |      Note that you should specify `total_sentences`; you'll run into problems if you ask to\n",
      " |      score more than this number of sentences but it is inefficient to set the value too high.\n",
      " |\n",
      " |      See the `article by Matt Taddy: \"Document Classification by Inversion of Distributed Language Representations\"\n",
      " |      <https://arxiv.org/pdf/1504.07295.pdf>`_ and the\n",
      " |      `gensim demo <https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/deepir.ipynb>`_ for examples of\n",
      " |      how to use such scores in document classification.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      total_sentences : int, optional\n",
      " |          Count of sentences.\n",
      " |      chunksize : int, optional\n",
      " |          Chunksize of jobs\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |\n",
      " |  seeded_vector(self, seed_string, vector_size)\n",
      " |\n",
      " |  train(self, corpus_iterable=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, compute_loss=False, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences.\n",
      " |\n",
      " |      Notes\n",
      " |      -----\n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |\n",
      " |      Warnings\n",
      " |      --------\n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once, you can set `epochs=self.epochs`.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      corpus_iterable : iterable of list of str\n",
      " |          The ``corpus_iterable`` can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network, to limit RAM usage.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          See also the `tutorial on data streaming in Python\n",
      " |          <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to`train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to `train()`.\n",
      " |          Use only if making multiple calls to `train()`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      compute_loss: bool, optional\n",
      " |          If True, computes and stores loss value which can be retrieved using\n",
      " |          :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      " |      callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          Sequence of callbacks to be executed at specific stages during training.\n",
      " |\n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |\n",
      " |          >>> from gensim.models import Word2Vec\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = Word2Vec(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)  # prepare the model vocabulary\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)  # train word vectors\n",
      " |          (1, 30)\n",
      " |\n",
      " |  update_weights(self)\n",
      " |      Copy all the existing weights, and reset the weights for the newly added vocabulary.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.utils.SaveLoad:\n",
      " |\n",
      " |  add_lifecycle_event(self, event_name, log_level=20, **event)\n",
      " |      Append an event into the `lifecycle_events` attribute of this object, and also\n",
      " |      optionally log the event at `log_level`.\n",
      " |\n",
      " |      Events are important moments during the object's life, such as \"model created\",\n",
      " |      \"model saved\", \"model loaded\", etc.\n",
      " |\n",
      " |      The `lifecycle_events` attribute is persisted across object's :meth:`~gensim.utils.SaveLoad.save`\n",
      " |      and :meth:`~gensim.utils.SaveLoad.load` operations. It has no impact on the use of the model,\n",
      " |      but is useful during debugging and support.\n",
      " |\n",
      " |      Set `self.lifecycle_events = None` to disable this behaviour. Calls to `add_lifecycle_event()`\n",
      " |      will not record events into `self.lifecycle_events` then.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event_name : str\n",
      " |          Name of the event. Can be any label, e.g. \"created\", \"stored\" etc.\n",
      " |      event : dict\n",
      " |          Key-value mapping to append to `self.lifecycle_events`. Should be JSON-serializable, so keep it simple.\n",
      " |          Can be empty.\n",
      " |\n",
      " |          This method will automatically add the following key-values to `event`, so you don't have to specify them:\n",
      " |\n",
      " |          - `datetime`: the current date & time\n",
      " |          - `gensim`: the current Gensim version\n",
      " |          - `python`: the current Python version\n",
      " |          - `platform`: the current platform\n",
      " |          - `event`: the name of this event\n",
      " |      log_level : int\n",
      " |          Also log the complete event dict, at the specified log level. Set to False to not log at all.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5cf20624-3cf0-4417-ba99-5d1bcdfcaa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2308 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'FastTextKeyedVectors' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[198], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_vector\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(vectorize_text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[1;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(df, df_function)(wrapper, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tqdm\\std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[0;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[196], line 3\u001b[0m, in \u001b[0;36mvectorize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvectorize_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     words \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m----> 3\u001b[0m     word_vectors \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39mwv(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(word_vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m word_vectors \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(model\u001b[38;5;241m.\u001b[39mget_dimension())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'FastTextKeyedVectors' object is not callable"
     ]
    }
   ],
   "source": [
    "df['text_vector'] = df['text'].progress_apply(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db9443-426d-4ad3-9d60-92685f1b4fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac25887-559f-4caa-8e64-4bda91cbd40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d8826-4cb4-4c87-8b97-d48a393c747e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3695171-abb5-488f-a6cb-32b7465a9ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
